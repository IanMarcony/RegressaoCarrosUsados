# -*- coding: utf-8 -*-
"""RegressaoCarrosUsados.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WqgoUyJ0KEe2in2ns_shDdBfSt2KcxuF

# Regressao usando base de Carros usados
"""

#Importar dados essenciais
import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive/')

dataset = pd.read_csv('/content/drive/My Drive/CursoIA/RegressaoCarrosUsados/autos.csv', encoding='ISO-8859-1')
dataset

#Removendo colunas de dataset que não tem relação com o preço do Veículo

dataset = dataset.drop('dateCrawled', axis=1)
dataset = dataset.drop('dateCreated', axis=1)
dataset = dataset.drop('postalCode', axis=1)
dataset = dataset.drop('nrOfPictures', axis=1)
dataset = dataset.drop('lastSeen', axis=1)

#Pelo motivo de haver vários nomes, vamos remover a coluna nome
dataset['name'].value_counts()

dataset = dataset.drop('name', axis=1)

#Pelo motivo de haver counas ainda que não tem muito impacto no nome, vamos remover estas colunas
dataset = dataset.drop('seller', axis=1)
dataset = dataset.drop('offerType', axis=1)

#Agora vamos pré-processar e remover valores inconsistentes
inconsistent = dataset.loc[dataset.price <=10]
print(len(inconsistent))

dataset = dataset.loc[dataset.price>10]

print(len(dataset))

inconsistent = dataset.loc[dataset.price>350000]
print(len(inconsistent))

dataset = dataset.loc[dataset.price<350000]
print(len(dataset))

#Removendo registros NAN
print(dataset['vehicleType'].value_counts()) #limousine
print(dataset['gearbox'].value_counts()) #manuell
print(dataset['model'].value_counts()) #golf
print(dataset['fuelType'].value_counts()) #benzin
print(dataset['notRepairedDamage'].value_counts()) #nein

values = {'vehicleType':'limousine',
          'gearbox':'manuell',
          'model':'golf',
          'fuelType':'benzin',
          'notRepairedDamage':'nein'}
dataset = dataset.fillna(value = values)

print(dataset)

#Dividindo previsores e preço

previsores = dataset.iloc[:, 1:13].values
preco_real = dataset.iloc[:, 0].values

#Importando Label Encoder

from sklearn.preprocessing import LabelEncoder

labelEncoder_previsores =  LabelEncoder()

previsores[:,0] = labelEncoder_previsores.fit_transform(previsores[:,0])
previsores[:,1] = labelEncoder_previsores.fit_transform(previsores[:,1])
previsores[:,3] = labelEncoder_previsores.fit_transform(previsores[:,3])
previsores[:,5] = labelEncoder_previsores.fit_transform(previsores[:,5])
previsores[:,8] = labelEncoder_previsores.fit_transform(previsores[:,8])
previsores[:,9] = labelEncoder_previsores.fit_transform(previsores[:,9])
previsores[:,10] = labelEncoder_previsores.fit_transform(previsores[:,10])

print(previsores[0])

#Importando OneHotEncoder

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0,1,3,5,8,9,10])],     remainder='passthrough')
previsores=columnTransformer.fit_transform(previsores).toarray()

previsores

# Hora de criar a rede neural
from keras.models import Sequential
from keras.layers import Dense

regressor = Sequential()

regressor.add(Dense(units = 158, activation = 'relu', input_dim = 316))
regressor.add(Dense(units = 158, activation = 'relu'))
regressor.add(Dense(units = 1, activation = 'linear'))
regressor.compile(loss = 'mean_absolute_error',
                  optimizer = 'adam',
                  metrics = ['mean_absolute_error'])

regressor.fit(previsores, preco_real, batch_size = 300, epochs = 100)

# Avaliando resultados

previsoes = regressor.predict(previsores)

print(preco_real.mean())
print(previsoes.mean())